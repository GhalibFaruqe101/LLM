{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419435\n",
      "['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'è', 'é', 'ê', 'ô', '—', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "# Ensure pandas is available in the environment if other cells need it\n",
    "# %pip install pandas\n",
    "\n",
    "with open('LLM/text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text))\n",
    "char = sorted(set(text))\n",
    "print(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f35d5c4ee305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Frankenstein;\n",
      "\n",
      "or, the Modern Prometheus\n",
      "\n",
      "by Mary Wollstonecraft (Godwin) Shelley\n",
      "\n",
      "\n",
      " CONTENTS\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf28c60d1eea637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\ufeff'], ['Frankenstein'], [';'], ['or'], [','], ['the'], ['Modern'], ['Prometheus'], ['by'], ['Mary'], ['Wollstonecraft'], ['('], ['Godwin'], [')'], ['Shelley'], ['CONTENTS'], ['Letter'], ['1'], ['Letter'], ['2'], ['Letter'], ['3'], ['Letter'], ['4'], ['Chapter'], ['1'], ['Chapter'], ['2'], ['Chapter'], ['3'], ['Chapter'], ['4'], ['Chapter'], ['5'], ['Chapter'], ['6'], ['Chapter'], ['7'], ['Chapter'], ['8'], ['Chapter'], ['9'], ['Chapter'], ['10'], ['Chapter'], ['11'], ['Chapter'], ['12'], ['Chapter'], ['13']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocess_data = re.split(r'([,.:;?_!\"()\\']|--|\\s)' ,text)\n",
    "preprocess_data = [item.split() for item in preprocess_data if item.strip()]\n",
    "print(preprocess_data[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73cefe8b0d2d0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84872\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocess_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfac6e149fce62f",
   "metadata": {},
   "source": [
    "creating vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7479ff3cbec2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7765\n"
     ]
    }
   ],
   "source": [
    "# preprocess_data is a list of lists (tokens), flatten and deduplicate to get a list of token strings\n",
    "words = sorted({token for item in preprocess_data for token in item})\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb361ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { token:integer for integer, token in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28276a5a7b20e05a",
   "metadata": {},
   "source": [
    "tokenizer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecfd3b8611a6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed =[ item if item in self.str_to_int\n",
    "                       else \"<unk>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2cf7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the vocabulary mapping (dict) rather than the list of tokens\n",
    "# 'vocab' is created in a later cell (index 12), so execute that cell before this one\n",
    "tokenizer = Tokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d5a264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"How slowly the time passes here, encompassed as I am by frost and snow!\n",
    "Yet a second step is taken towards my enterprise. I have hired a\n",
    "vessel and am occupied in collecting my sailors; those whom I have\n",
    "already engaged appear to be men on whom I can depend and are certainly\n",
    "possessed of dauntless courage.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bab7e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299, 6276, 6757, 6833, 5033, 3617, 4, 2691, 1056, 303, 906, 1496, 3276, 933, 6293, 0, 672, 681, 6035, 6460, 4109, 6675, 6889, 4696, 2753, 5, 303, 3561, 3652, 681, 7212, 933, 906, 4847, 3841, 1735, 4696, 5953, 46, 6793, 7399, 303, 3561, 896, 2723, 985, 6841, 1226, 4508, 4881]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63bb05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(words)))\n",
    "all_tokens.extend([\"<unk>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bdaa5",
   "metadata": {},
   "source": [
    "BytePair encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9c5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018000dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daffc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_text = \"\"\"How slowly the txt time passes here, encompassed as I am by frost and snow!\n",
    "Yet a second step is taken towards my enterprise. I have hired a\n",
    "vessel and am occupied in collecting my sailors; those whom I have\n",
    "already engaged appear to be men on whom I can depend and are certainly\n",
    "possessed of dauntless courage.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d022c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2437, 6364, 262, 256, 742, 640, 8318, 994, 11, 20504, 21390, 355, 314, 716, 416, 21682, 290, 6729, 0, 198, 11486, 257, 1218, 2239, 318, 2077, 3371, 616, 13953, 13, 314, 423, 9657, 257, 198, 1158, 741, 290, 716, 12030, 287, 13157, 616, 29996, 26, 883, 4150, 314, 423, 198]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(n_text, allowed_special={\"<|unk>|\"})[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30433f",
   "metadata": {},
   "source": [
    "Data Sampling using sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006b9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0019c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7176d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de1f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLLM/text.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     raw_text = f.read()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     enc_text= tokenizer.encode(\u001b[43mtext\u001b[49m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(enc_text ))\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# with open('LLM/text.txt', 'r', encoding='utf-8') as f:\n",
    "#     raw_text = f.read()\n",
    "#     enc_text= tokenizer.encode(text)\n",
    "#     print(len(enc_text ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d01341",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[1000:2000]\n",
    "# x=enc_sample[:4]\n",
    "# y=enc_sample[1:4+1]\n",
    "# dec_x= tokenizer.decode(x)\n",
    "# dec_y= tokenizer.decode(y)\n",
    "\n",
    "# print(f\"x: {dec_x}\")\n",
    "# print(f\"y: {dec_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,4+1):\n",
    "#     context = enc_sample[:i]\n",
    "#     desired = enc_sample[i]\n",
    "#     print(tokenizer.decode(context), '--> ',tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc5e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5dbcab",
   "metadata": {},
   "source": [
    "Dataset\n",
    "- stride --> controled overlap (between cunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f1080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_V1(Dataset):\n",
    "    def __init__(self,txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "#tokenizing the text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={ \"<|endoftext>\"})\n",
    "        assert len(token_ids) > max_length, \"tokenized input equal or +1\"\n",
    "#sliding window\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e3b56",
   "metadata": {},
   "source": [
    "dataloader\n",
    "- txt --> raw input\n",
    "- batch_size --> no of samples\n",
    "- drop_batch --> whether to drop last batch if is smaller than batch_size\n",
    "- num_workers --> no of subprocessees for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7a42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):  \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = Dataset_V1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece377d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LLM/text.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b7496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3f8b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  171,   119,   123,   198],\n",
      "        [  198, 17439, 37975,    26],\n",
      "        [  198,   198,   273,    11],\n",
      "        [  262, 12495, 42696,   198],\n",
      "        [  198,  1525,  5335,   370],\n",
      "        [  692,  6440,  3323,   357],\n",
      "        [13482,  5404,     8, 46854],\n",
      "        [  628,   198, 22904, 15365]]) torch.Size([8, 4])\n",
      "Targets:\n",
      " tensor([[  119,   123,   198,   198],\n",
      "        [17439, 37975,    26,   198],\n",
      "        [  198,   273,    11,   262],\n",
      "        [12495, 42696,   198,   198],\n",
      "        [ 1525,  5335,   370,   692],\n",
      "        [ 6440,  3323,   357, 13482],\n",
      "        [ 5404,     8, 46854,   628],\n",
      "        [  198, 22904, 15365,   628]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "input, target = next(data_iter)\n",
    "print(\"Inputs:\\n\", input, input.shape)\n",
    "print(\"Targets:\\n\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736609b4",
   "metadata": {},
   "source": [
    "embeding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a96122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50247\n",
    "output_dim =256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embedding = token_embedding_layer(input)\n",
    "context_length = 4\n",
    "pos_embedding = torch.nn.Embedding(context_length, output_dim)\n",
    "print(token_embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
